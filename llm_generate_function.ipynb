{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e3108a-f0c0-437e-80cc-a208d9b3d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1112082/Desktop/openAI/env/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import time\n",
    "model_name = 'deberain/gpt-neo-125M-fine-tuned-on-ChatGPT-tweets'\n",
    "#model_name = '/Users/1112082/Desktop/openAI/llama2/llamas/gpt-neo_train/pretrained'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "EOS_TOKEN_ID = 50256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cedfd7e-461b-47da-b1d3-f663b639dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm1(prompt, max_new_tokens, do_sample, temperature, top_k, top_p, repetition_penalty):\n",
    "    start_time = time.time()\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_ids=model_inputs['input_ids'], \n",
    "        pad_token_id=EOS_TOKEN_ID,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    gen_txt = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(gen_txt,'\\nelapsed time:',time.time()-start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b140debf-d825-4e80-8064-2a49c30c0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder2(token_ids):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    return ''.join(tokens).replace('Ġ',' ').replace('Ċ','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9a4d0b3-6788-4ab2-b637-4eea5293ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(\n",
    "    prompt, \n",
    "    max_new_tokens=20, \n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    top_k = 0,\n",
    "    top_p = 1,\n",
    "    repetition_penalty = 1\n",
    "):\n",
    "    start_time = time.time()\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    gen_token_ids = model_inputs['input_ids']\n",
    "\n",
    "    output_attentions = ()\n",
    "    output_hidden_states = ()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #iterate up to 'max_new_tokens'\n",
    "        for i in range(max_new_tokens):\n",
    "            inp = model.forward(\n",
    "                gen_token_ids\n",
    "            )\n",
    "            \n",
    "            #repetition penalty process\n",
    "            logits = inp.logits[0][-1]\n",
    "            score  = torch.gather(logits, 0, gen_token_ids[0])\n",
    "            score  = torch.where(score < 0, score * repetition_penalty, score / repetition_penalty)\n",
    "            logits = logits.scatter_(0,gen_token_ids[0],score)\n",
    "            \n",
    "            #if 'do_sample' is True\n",
    "            if do_sample:\n",
    "                logits = logits / temperature #'temperature' works only in case 'do_sample' is True            \n",
    "                if top_k > 0:\n",
    "                    indices_to_remove = logits < torch.min( torch.topk(logits, top_k)[0] )\n",
    "                    logits = logits.masked_fill(indices_to_remove, -float(\"Inf\"))            \n",
    "                if 0 <= top_p <= 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=False)\n",
    "                    cumulative_probs = sorted_logits.softmax(dim=0).cumsum(dim=0)\n",
    "                    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove) # dim, index, value                \n",
    "                    logits = logits.masked_fill(indices_to_remove, -float(\"Inf\"))\n",
    "                probs = logits.softmax(dim=0) \n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(0)\n",
    "            else:\n",
    "                logits = logits.softmax(dim=0)\n",
    "                token_id = torch.argmax(logits, dim=0)\n",
    "            \n",
    "            #EOS Token ID 가 예측되면 max_new_tokens 에 도달하지 못했어도 종료\n",
    "            if token_id == EOS_TOKEN_ID:\n",
    "                break\n",
    "            gen_token_ids = torch.cat( (gen_token_ids, torch.tensor([[token_id]]) ), dim=1)\n",
    "        \n",
    "    gen_txt = decoder2(gen_token_ids[0])\n",
    "    print(gen_txt,'\\nelapsed time:',time.time()-start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dce0b423-2ae5-4b02-8255-33b688feb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is gravity\"\n",
    "prompt_template = \"Question: {prompt}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6d17f74-5e12-40c3-b933-d224542df65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is gravity\n",
      "A: Gravity in air has an average of 1.5, while in water it can range from 0 to 3/2! \n",
      "elapsed time: 0.6250591278076172\n",
      "=============\n",
      "Q: What is gravity\n",
      "A: Gravity occurs when an object is thrown at its gravitational acceleration. \n",
      "elapsed time: 0.7758758068084717\n"
     ]
    }
   ],
   "source": [
    "#llm1 : 원본 , llm2 & decoder2 : Custom. \n",
    "llm1(prompt_template.format(prompt=prompt), max_new_tokens=50,do_sample=True,temperature=1, top_k = 50, top_p=0.9, repetition_penalty=10.0)\n",
    "print('=============')\n",
    "llm2(prompt_template.format(prompt=prompt), max_new_tokens=50,do_sample=True,temperature=1, top_k = 50, top_p=0.9, repetition_penalty=10.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
